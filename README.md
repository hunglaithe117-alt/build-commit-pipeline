# Build Commit Pipeline

Pipeline orchestrator for TravisTorrent data ingestion and SonarQube enrichment with **distributed instance pooling**.

## Features

- **Multi-Instance SonarQube Pool**: Process commits across multiple SonarQube instances in parallel
- **Redis Distributed Locks**: Ensure per-instance concurrency = 1 while allowing global concurrency > 1
- **High Throughput**: Process multiple commits simultaneously with automatic load balancing
- **Fault Tolerant**: Auto-retry with exponential backoff, auto-expiring locks prevent deadlocks
- **Observable**: Real-time instance status monitoring via REST API

## Architecture

```
Celery Workers (concurrency=4) 
    ‚Üì
Redis Lock Manager (round-robin)
    ‚Üì
SonarQube Instances (2+)
    - primary: localhost:9001
    - secondary: sonarqube2:9002
```

**Key Principle**: Global concurrency > 1, but each SonarQube instance handles max 1 job at a time (SonarQube CE limitation).

## Quick Start

```bash
# Start all services
docker-compose up -d

# Monitor workers
Use the API or container logs to observe parallel sonar-scanner workers. For example:

```bash
# View worker logs
docker-compose logs -f worker
```

# View worker logs
docker-compose logs -f worker
```

See [INSTANCE_POOL_QUICKSTART.md](./docs/INSTANCE_POOL_QUICKSTART.md) for detailed usage.

## Documentation

- [Instance Pool Architecture](./docs/INSTANCE_POOL_ARCHITECTURE.md) - Detailed system design
- [Quick Start Guide](./docs/INSTANCE_POOL_QUICKSTART.md) - Usage and operations

## API Endpoints

### Sonar / Jobs API
The primary API for pipeline control is focused on queuing jobs and viewing scan/export results. Use the `jobs`, `sonar` and `outputs` endpoints to submit work and inspect results.

### Data Sources & Jobs
- `GET /api/data-sources` - List data sources
- `POST /api/jobs` - Create analysis job
- `GET /api/jobs` - List jobs
- `GET /api/outputs` - List output files

## Performance

| Commits | Concurrency | Time (avg 2min/commit) |
|---------|-------------|------------------------|
| 100     | 1           | ~200 minutes (~3.3h)   |
| 100     | 4           | ~50 minutes            |
| 100     | 8           | ~25 minutes            |
| 100     | 16          | ~13 minutes            |

**Speedup: Up to 16x faster with parallel scanners!** üöÄ

## Resource Requirements

| Concurrency | RAM   | CPU  | Disk    |
|-------------|-------|------|---------|
| 4           | 4GB   | 4    | 50GB    |
| 8           | 8GB   | 8    | 100GB   |
| 16          | 16GB  | 16   | 200GB   |

## Configuration Options

### Increase Concurrency

```yaml
# config/pipeline.yml
pipeline:
  sonar_parallelism: 16  # More parallel scanners
```

### Tune SonarQube Performance

```yaml
# docker-compose.yml
sonarqube:
  environment:
    SONAR_CE_JAVAOPTS: "-Xmx8g -Xms4g"  # More memory
    SONAR_WEB_JAVAADDITIONALOPTS: "-Dsonar.web.http.maxThreads=300"  # More threads
```

## Technology Stack

- **FastAPI** - REST API framework
- **Celery** - Distributed task queue  
- **RabbitMQ** - Message broker
- **MongoDB** - Job/metadata storage
- **SonarQube** - Code quality analysis server
- **PostgreSQL** - SonarQube metrics database
- **Docker** - Containerization

## Monitoring

```bash
# Watch parallel scanners in action
docker-compose logs -f worker | grep "Processing commit"

# Monitor resource usage
docker stats worker sonarqube

# Check queue depth
curl -u pipeline:pipeline http://localhost:15672/api/queues
```

## Troubleshooting

### Workers idle, no scanning

```bash
# Check RabbitMQ connection
docker-compose logs worker | grep -i connected

# Restart worker
docker-compose restart worker
```

### SonarQube out of memory

```bash
# Check memory usage
docker stats sonarqube

# Increase heap size in docker-compose.yml
SONAR_CE_JAVAOPTS: "-Xmx8g -Xms4g"
```

### Disk space full

```bash
# Clean old worktrees
docker-compose exec worker find /app/data/sonar-work -name worktrees -exec rm -rf {} +
```

## License

MIT


Pipeline thu th·∫≠p & l√†m gi√†u TravisTorrent v·ªõi FastAPI + Celery + RabbitMQ + MongoDB, t√≠ch h·ª£p SonarQube webhook v√† giao di·ªán Next.js ƒë·ªÉ qu·∫£n l√Ω to√†n b·ªô quy tr√¨nh.

## Ki·∫øn tr√∫c t·ªïng quan

```
frontend (Next.js)
    ‚îî‚îÄ‚îÄ g·ªçi REST API ƒë·ªÉ upload CSV, gi√°m s√°t job, t·∫£i metrics
backend (FastAPI)
    ‚îú‚îÄ‚îÄ API ƒë·ªìng b·ªô (upload CSV, trigger job, li·ªát k√™ SonarQube runs, t·∫£i output)
    ‚îú‚îÄ‚îÄ Celery worker x·ª≠ l√Ω ingestion + scan + export metrics
    ‚îú‚îÄ‚îÄ RabbitMQ l√†m broker/queue + Dead Letter Queue (Mongo)
    ‚îú‚îÄ‚îÄ MongoDB l∆∞u metadata dataset, job queue, DLQ, ƒë∆∞·ªùng d·∫´n output
    ‚îî‚îÄ‚îÄ Module `pipeline/sonar.py` t√°i hi·ªán logic c·ªßa `sonar_scan_csv_multi.py` ƒë·ªÉ clone repo, checkout commit v√† ch·∫°y sonar-scanner
SonarQube
    ‚îî‚îÄ‚îÄ Kh·ªüi ch·∫°y b·∫±ng docker-compose.sonarqube.yml (th∆∞ m·ª•c sonar-scan/) v√† c·∫•u h√¨nh webhook ‚Üí backend
Observability
    ‚îî‚îÄ‚îÄ Grafana Loki + Promtail + Grafana theo d√µi stdout containers v√† file log trong `data/`
```

## Th∆∞ m·ª•c quan tr·ªçng

- `backend/` ‚Äì FastAPI app (`app/main.py`), c·∫•u h√¨nh Celery (`app/celery_app.py`), service layer (`app/services/*`), pipelines (`backend/pipeline/*`).
- `frontend/` ‚Äì Next.js 14 app cung c·∫•p 4 m√†n h√¨nh: ngu·ªìn d·ªØ li·ªáu, job thu th·∫≠p, SonarQube runs, output.
- `config/pipeline.yml` ‚Äì YAML c·∫•u h√¨nh duy nh·∫•t cho k·∫øt n·ªëi Mongo/RabbitMQ, ƒë∆∞·ªùng d·∫´n Sonar script, c√°c metric keys mu·ªën export.
- `docker-compose.yml` ‚Äì Kh·ªüi ch·∫°y API + worker + beat + frontend + RabbitMQ + Mongo. M·∫∑c ƒë·ªãnh mount th∆∞ m·ª•c `../sonar-scan` ƒë·ªÉ t√°i s·ª≠ d·ª•ng c√°c script hi·ªán c√≥, ƒë·ªìng th·ªùi t·∫°o hai database Postgres ri√™ng cho t·ª´ng SonarQube instance.
- `config/postgres-init.sql` ‚Äì Script kh·ªüi t·∫°o `sonar_primary` v√† `sonar_secondary` ƒë·ªÉ m·ªói SonarQube d√πng database ri√™ng, tr√°nh xung ƒë·ªôt migration.
- `data/` ‚Äì L∆∞u file upload, dead-letter artifact, v√† CSV metrics sau khi export (ƒë∆∞·ª£c mount v√†o containers).

## Quick start (ch·∫°y nhanh)

Ch·∫°y to√†n b·ªô stack b·∫±ng Docker (g·ªìm API, worker, frontend, RabbitMQ, Mongo, SonarQube n·∫øu b·∫°n c√≥ c·∫•u h√¨nh):

```bash
cp .env.example .env                            # sau ƒë√≥ ch·ªânh APP_UID/APP_GID theo m√°y c·ªßa b·∫°n
# ho·∫∑c m·ªôt d√≤ng: APP_UID=$(id -u) APP_GID=$(id -g) envsubst < .env.example > .env
# ch·ªânh token SonarQube trong config/pipeline.yml tr∆∞·ªõc khi kh·ªüi ƒë·ªông
docker compose up --build
```

Ch·ªâ ch·∫°y backend c·ª•c b·ªô (ph√°t tri·ªÉn API):

```bash
cd backend
curl -LsSf https://astral.sh/uv/install.sh | sh  # n·∫øu ch∆∞a c√≥ uv
uv sync --frozen --no-dev                        # t·∫°o .venv theo lockfile
source .venv/bin/activate
uv run uvicorn app.main:app --reload
# ch·∫°y celery worker trong terminal kh√°c
uv run celery -A app.celery_app.celery_app worker -l info
```

Frontend (c·ª•c b·ªô):

```bash
cd frontend
npm install
npm run dev
```

## Troubleshooting

- SonarQube kh√¥ng g·ª≠i webhook: ki·ªÉm tra `sonarqube.webhook_secret` trong `config/pipeline.yml` v√† ƒë·∫£m b·∫£o endpoint `http://<host>:8000/api/sonar/webhook` c√≥ th·ªÉ truy c·∫≠p t·ª´ SonarQube container.
- Celery kh√¥ng th·ª±c thi task: ki·ªÉm tra broker (RabbitMQ) URL v√† r·∫±ng worker ƒëang ch·∫°y (`uv run celery -A app.celery_app.celery_app worker -l info`).
- K·∫øt n·ªëi Mongo th·∫•t b·∫°i: ki·ªÉm tra chu·ªói k·∫øt n·ªëi trong `config/pipeline.yml` v√† ƒë·∫£m b·∫£o Mongo ƒë√£ kh·ªüi ƒë·ªông tr∆∞·ªõc khi API k·∫øt n·ªëi.
- SonarScanner kh√¥ng ch·∫°y: ƒë·∫£m b·∫£o SonarScanner CLI c√≥ s·∫µn tr√™n host/container v√† m·ªói instance trong `config/pipeline.yml` c√≥ token h·ª£p l·ªá.

## Chu·∫©n b·ªã

1. **Ch·∫°y SonarQube**: d√πng `sonar-scan/docker-compose.sonarqube.yml` nh∆∞ b·∫°n ƒë√£ c√≥ ƒë·ªÉ b·∫≠t SonarQube v√† SonarScanner CLI.
2. **T·∫°o `.env`**: sao ch√©p `.env.example` th√†nh `.env`, ƒë·∫∑t `APP_UID` v√† `APP_GID` (th∆∞·ªùng l√† k·∫øt qu·∫£ c·ªßa `id -u` v√† `id -g`). Docker Compose s·∫Ω ch·∫°y c√°c service backend b·∫±ng UID/GID n√†y ƒë·ªÉ m·ªçi file trong `./data` lu√¥n thu·ªôc s·ªü h·ªØu user hi·ªán t·∫°i, kh√¥ng ph·∫£i ch·∫°y `sudo chown` sau m·ªói l·∫ßn pull. N·∫øu tr∆∞·ªõc ƒë√¢y th∆∞ m·ª•c `data/` ƒë√£ b·ªã root chi·∫øm quy·ªÅn, ch·ªâ c·∫ßn `sudo chown -R $(id -u):$(id -g) data` **m·ªôt l·∫ßn** ƒë·ªÉ ƒë·ªìng b·ªô l·∫°i.
3. **ƒêi·ªÅn config**:
   - Sao ch√©p `config/pipeline.example.yml` th√†nh `config/pipeline.yml` (ƒë√£ th·ª±c hi·ªán v·ªõi c·∫•u h√¨nh m·∫∑c ƒë·ªãnh). C·∫≠p nh·∫≠t:
     - `sonarqube.instances`: danh s√°ch SonarQube b·∫°n mu·ªën d√πng (m·ªói entry c·∫ßn `host` v√† `token`). Worker s·∫Ω round-robin commit qua c√°c instance n√†y.
     - `sonarqube.webhook_secret`: chu·ªói b√≠ m·∫≠t ƒë·ªÉ SonarQube g·ª≠i webhook.
4. **Logging (t√πy ch·ªçn)**: N·∫øu s·ª≠ d·ª•ng Loki + Promtail + Grafana trong `docker-compose.yml`, gi·ªØ nguy√™n `config/promtail-config.yml` ho·∫∑c ch·ªânh l·∫°i ƒë∆∞·ªùng log mong mu·ªën.

## Backend d√πng uv

To√†n b·ªô dependencies Python ƒë∆∞·ª£c qu·∫£n l√Ω b·∫±ng [uv](https://github.com/astral-sh/uv) (ƒë√£ kh√≥a trong `backend/uv.lock`). L√†m vi·ªác c·ª•c b·ªô:

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh  # n·∫øu ch∆∞a c√≥ uv
cd build-commit-pipeline/backend
uv sync --frozen --no-dev                        # t·∫°o .venv theo lockfile
source .venv/bin/activate                        # ho·∫∑c d√πng `uv run ...`
uv run uvicorn app.main:app --reload
uv run celery -A app.celery_app.celery_app worker -l info
```

Dockerfile backend c≈©ng s·ª≠ d·ª•ng `uv sync --frozen` n√™n build lu√¥n b√°m s√°t `uv.lock`.

## Ch·∫°y to√†n b·ªô stack

```bash
cd build-commit-pipeline
docker compose up --build
```

- API: <http://localhost:8000>
- Frontend: <http://localhost:3000>
- Mongo: mongodb://travis:travis@localhost:27017 (authSource=admin)
- RabbitMQ: amqp://pipeline:pipeline@localhost:5672//

## Quy tr√¨nh s·ª≠ d·ª•ng giao di·ªán

1. **Ngu·ªìn d·ªØ li·ªáu** (`/data-sources`)
   - Upload file CSV (v√≠ d·ª• t·ª´ `19314170/ruby_per_project_csv`). Backend t·ª± ƒë·ªông t√≥m t·∫Øt s·ªë build/commit, t·∫°o record trong Mongo.
   - B·∫•m "Thu th·∫≠p d·ªØ li·ªáu" ƒë·ªÉ queue job Celery (`ingest_data_source`). C√°c commit trong CSV s·∫Ω ƒë∆∞·ª£c ƒë∆∞a v√†o h√†ng ƒë·ª£i v√† ph√¢n ph·ªëi l·∫ßn l∆∞·ª£t cho t·ª´ng SonarQube instance, m·ªói instance ch·ªâ ch·∫°y t·ªëi ƒëa 1 commit (Community) t·∫°i m·ªôt th·ªùi ƒëi·ªÉm.

2. **Thu th·∫≠p** (`/jobs`)
   - Theo d√µi tr·∫°ng th√°i job (queued/running/succeeded/failed), s·ªë commit ƒë√£ x·ª≠ l√Ω / t·ªïng v√† commit ƒëang ch·∫°y. Progress bar c·∫≠p nh·∫≠t m·ªói 5 gi√¢y v·ªõi d·ªØ li·ªáu realtime t·ª´ Mongo.

3. **SonarQube runs** (`/sonar-runs`)
   - Hi·ªÉn th·ªã t·ª´ng commit ƒë√£ submit l√™n SonarQube (component key = `{project}_{commit}`), tr·∫°ng th√°i webhook, log file path v√† ƒë∆∞·ªùng d·∫´n metrics sau khi export. Khi webhook b√°o th√†nh c√¥ng, Celery s·∫Ω t·ª± ƒë·ªông g·ªçi task `export_metrics` ƒë·ªÉ tr√≠ch xu·∫•t measures v√† l∆∞u file CSV v√†o `data/exports`.

4. **D·ªØ li·ªáu ƒë·∫ßu ra** (`/outputs`)
   - Li·ªát k√™ c√°c b·ªô metric ƒë√£ ƒë∆∞·ª£c export. C√≥ link t·∫£i nhanh `api/outputs/{id}/download`.

## Scale nhi·ªÅu SonarQube instance

Trong `config/pipeline.yml`, b·∫°n c√≥ th·ªÉ khai b√°o nhi·ªÅu instance:

```yaml
sonarqube:
  instances:
    - name: primary
      host: http://sonarqube1:9000
      token: "token-primary"
    - name: secondary
      host: http://sonarqube2:9000
      token: "token-secondary"
```

M·ªói commit t·ª´ CSV s·∫Ω ƒë∆∞·ª£c g√°n l·∫ßn l∆∞·ª£t cho t·ª´ng instance. Th√¥ng tin `sonar_instance`, `sonar_host`, commit hi·ªán t·∫°i v√† log file ƒë·ªÅu ƒë∆∞·ª£c hi·ªÉn th·ªã tr√™n giao di·ªán `/jobs` v√† `/sonar-runs` ƒë·ªÉ d·ªÖ theo d√µi realtime.

H·ªá th·ªëng hi·ªán v·∫≠n h√†nh theo m√¥ h√¨nh m·ªôt SonarQube server thu nh·∫≠n c√°c ph√¢n t√≠ch, v√† nhi·ªÅu sonar-scanner worker ch·∫°y song song ƒë·ªÉ submit analyses. SonarQube (CE) x·ª≠ l√Ω m·ªôt ph√¢n t√≠ch t·∫°i m·ªôt th·ªùi ƒëi·ªÉm; khi s·ª≠ d·ª•ng m·ªôt server duy nh·∫•t, vi·ªác ph√¢n ph·ªëi work ƒë∆∞·ª£c th·ª±c hi·ªán b·ªüi h√†ng ƒë·ª£i Celery v√† nhi·ªÅu worker ch·∫°y ƒë·ªìng th·ªùi.
- Docker Compose ƒë√£ c·∫•u h√¨nh s·∫µn hai database Postgres (`sonar_primary`, `sonar_secondary`) th√¥ng qua `config/postgres-init.sql`, v√¨ v·∫≠y m·ªói SonarQube container s·ª≠ d·ª•ng schema ri√™ng bi·ªát v√† kh√¥ng tranh ch·∫•p migration. N·∫øu b·∫°n ƒë√£ ch·∫°y phi√™n b·∫£n c≈© (m·ªôt database), h√£y x√≥a volume `postgres_data` tr∆∞·ªõc khi kh·ªüi ƒë·ªông l·∫°i ƒë·ªÉ script c√≥ c∆° h·ªôi t·∫°o schema m·ªõi.

## Observability (Grafana + Loki)

- `docker-compose.yml` b·ªï sung 3 d·ªãch v·ª•:
  - `loki` (port 3100) l∆∞u tr·ªØ log.
  - `promtail` tail stdout c·ªßa Docker (`/var/lib/docker/containers/*`) v√† c√°c file log trong `data/` (nh∆∞ `sonar-work/*/logs/*.log`, `dead_letter/*.json`, `error_logs/*.log`) theo c·∫•u h√¨nh `config/promtail-config.yml`.
  - `grafana` (port 3001, admin/admin) ƒë·ªÉ tr·ª±c quan h√≥a.
- Sau khi `docker compose up -d loki promtail grafana`, v√†o Grafana ‚Üí add data source ‚Üí Loki (`http://loki:3100`).
- C√°c nh√£n log quan tr·ªçng:
  - `job="docker-containers"`: log stdout c·ªßa API, Celery worker/beat, frontend, RabbitMQ, Mongo, SonarQube, v.v.
  - `job="sonar-commit-logs"`: log t·ª´ng commit (`data/sonar-work/<instance>/<project>/logs/*.log`).
  - `job="dead-letter"`: JSON payload commit l·ªói trong `data/dead_letter`.
  - `job="pipeline-error-files"`: file `data/error_logs/*.log`.
- N·∫øu mu·ªën b·ªï sung ƒë∆∞·ªùng log kh√°c (v√≠ d·ª• upload ti·∫øn ƒë·ªô), ch·ªânh `config/promtail-config.yml` v√† reload Promtail.

## Hook SonarQube webhook

1. Trong SonarQube ‚Üí Administration ‚Üí Configuration ‚Üí Webhooks ‚Üí Add:
   - **URL**: `http://host-may-ban:8000/api/sonar/webhook`
   - **Secret**: d√πng gi√° tr·ªã `sonarqube.webhook_secret` trong YAML.
2. Sau m·ªói analysis th√†nh c√¥ng, SonarQube s·∫Ω POST payload. Backend x√°c th·ª±c ch·ªØ k√Ω (`X-Sonar-Webhook-HMAC-SHA256` ho·∫∑c `X-Sonar-Secret`). N·∫øu status = OK/SUCCESS, Celery `export_metrics` ch·∫°y ngay, ghi ƒë∆∞·ªùng d·∫´n v√†o Mongo + outputs.

## Dead Letter Queue

- Khi Celery task th·∫•t b·∫°i (v√≠ d·ª• Sonar scan l·ªói), backend ghi l·∫°i payload v√†o collection `dead_letters` v√† tr·∫°ng th√°i data source chuy·ªÉn `failed`.
- File log/chi ti·∫øt c≈©ng c√≥ th·ªÉ ghi ra `data/dead_letter/` n·∫øu c·∫ßn m·ªü r·ªông (`LocalFileService`).

## T√≠ch h·ª£p script hi·ªán t·∫°i

- **Scanning**: `pipeline/sonar.py` chuy·ªÉn logic t·ª´ `sonar-scan/sonar_scan_csv_multi.py` v√†o Python module. M·ªôt SonarCommitRunner ƒë∆∞·ª£c t·∫°o cho t·ª´ng instance v√† CSV; runner clone repo, checkout t·ª´ng commit tu·∫ßn t·ª± v√† ch·∫°y `sonar-scanner`.
- **Metrics export**: `pipeline/sonar.py::MetricsExporter` l·∫•y c·∫£m h·ª©ng t·ª´ `sonar-scan/batch_fetch_all_measures.py`, nh∆∞ng g√≥i g·ªçn cho t·ª´ng project key, chunk metric theo YAML.
- N·∫øu mu·ªën ch·∫°y h√†ng lo·∫°t, ch·ªâ c·∫ßn ƒë·∫∑t nhi·ªÅu file CSV trong th∆∞ m·ª•c `data/uploads/` r·ªìi queue nhi·ªÅu data source.

## API ch√≠nh (FastAPI)

| Method | Path | M√¥ t·∫£ |
|--------|------|-------|
| `POST /api/data-sources?name=` | Upload CSV (multipart). Tr·∫£ v·ªÅ metadata + stats. |
| `POST /api/data-sources/{id}/collect` | Queue job Celery ƒë·ªÉ scan + l·∫•y metrics. |
| `GET /api/jobs` | Danh s√°ch job ingest v·ªõi ph√¢n trang. |
| `GET /api/jobs/workers-stats` | **M·ªöI**: Th·ªëng k√™ workers v√† tasks ƒëang ch·∫°y realtime. |
| `GET /api/sonar/runs` | L·ªãch s·ª≠ webhook/scan. |
| `POST /api/sonar/webhook` | Endpoint nh·∫≠n webhook SonarQube. |
| `GET /api/outputs` | Danh s√°ch dataset enriched. |
| `GET /api/outputs/{id}/download` | T·∫£i file metrics CSV. |

## Worker Monitoring (T√≠nh nƒÉng m·ªõi) üÜï

H·ªá th·ªëng hi·ªán h·ªó tr·ª£ theo d√µi workers realtime tr√™n trang Jobs:

### Th√¥ng tin hi·ªÉn th·ªã:
- **S·ªë Workers**: T·ªïng s·ªë Celery workers ƒëang ho·∫°t ƒë·ªông
- **Concurrency (max)**: S·ªë task t·ªëi ƒëa c√≥ th·ªÉ ch·∫°y ƒë·ªìng th·ªùi
- **ƒêang scan**: S·ªë commits ƒëang ƒë∆∞·ª£c scan ngay l√∫c n√†y
- **ƒêang ch·ªù**: S·ªë commits trong queue ch·ªù x·ª≠ l√Ω

### Chi ti·∫øt t·ª´ng Worker:
M·ªói worker hi·ªÉn th·ªã:
- T√™n worker (v√≠ d·ª•: `celery@sonar-worker-1`)
- S·ªë tasks ƒëang ch·∫°y / t·ªëi ƒëa
- Th√¥ng tin t·ª´ng task ƒëang ch·∫°y:
  - Commit SHA (8 k√Ω t·ª±)
  - Repository/Project key

### C·∫•u h√¨nh Worker Concurrency:

```yaml
# config/pipeline.yml
pipeline:
  sonar_parallelism: 8  # S·ªë workers/tasks ch·∫°y ƒë·ªìng th·ªùi
```

**V√≠ d·ª• hi·ªáu su·∫•t:**
- `sonar_parallelism: 4` ‚Üí 4 commits scan c√πng l√∫c
- `sonar_parallelism: 8` ‚Üí 8 commits scan c√πng l√∫c
- `sonar_parallelism: 16` ‚Üí 16 commits scan c√πng l√∫c

### API Endpoint M·ªõi:

**GET /api/jobs/workers-stats**

Tr·∫£ v·ªÅ th√¥ng tin realtime v·ªÅ workers:

```json
{
  "total_workers": 2,
  "max_concurrency": 8,
  "active_scan_tasks": 5,
  "queued_scan_tasks": 10,
  "workers": [
    {
      "name": "celery@worker1",
      "active_tasks": 3,
      "max_concurrency": 8,
      "tasks": [
        {
          "id": "task-uuid",
          "name": "app.tasks.sonar.run_commit_scan",
          "current_commit": "abc123def",
          "current_repo": "owner/repo"
        }
      ]
    }
  ]
}
```

### T√†i li·ªáu chi ti·∫øt:

- [H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng (Ti·∫øng Vi·ªát)](./docs/HUONG_DAN_SU_DUNG.md) - H∆∞·ªõng d·∫´n ƒë·∫ßy ƒë·ªß t·ª´ A-Z
- [Worker Monitoring Documentation](./docs/WORKER_MONITORING.md) - Chi ti·∫øt k·ªπ thu·∫≠t v·ªÅ worker monitoring

## M·ªü r·ªông

- Th√™m `app/tasks/sonar.py` ƒë·ªÉ h·ªó tr·ª£ queue retry th·ªß c√¥ng ho·∫∑c cron refresh.
- D·ªÖ d√†ng chuy·ªÉn sang message broker kh√°c (v√≠ d·ª• ƒë·ªïi RabbitMQ host) b·∫±ng c√°ch ch·ªânh `broker.url` trong YAML + Celery config.
- C√≥ th·ªÉ th√™m trang qu·∫£n l√Ω DLQ b·∫±ng c√°ch ƒë·ªçc collection `dead_letters`.
